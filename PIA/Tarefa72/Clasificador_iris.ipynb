{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "irisDataset = pd.read_csv(\"iris.data\", names=[\"sepallength\",\"sepalwidth\",\"petallength\",\"petalwidth\",\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase para escalar los datos (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mean: {self.mean}, std:{self.std}, epsilon:{self.epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets y Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "  def __init__(self, src_file, root_dir, transform=None):\n",
    "    irisDataset = pd.read_csv(src_file, names=[\"sepallength\",\"sepalwidth\",\"petallength\",\"petalwidth\",\"class\"])\n",
    "\n",
    "    X = irisDataset[irisDataset.columns.intersection([\"sepallength\",\"sepalwidth\",\"petallength\",\"petalwidth\"])]\n",
    "    Y = irisDataset[irisDataset.columns.intersection([\"class\"])]\n",
    "\n",
    "    nomeClases = Y[\"class\"].unique()\n",
    "    conversion = {v: k for k, v in dict(enumerate(nomeClases)).items()}\n",
    "    YConversion = pd.DataFrame()\n",
    "\n",
    "    for nome in nomeClases:\n",
    "      YConversion[nome] = (Y[\"class\"]==nome).apply(lambda x : 1.0 if x else 0.0)\n",
    "      \n",
    "    y_tensor = torch.as_tensor(YConversion.to_numpy()).type(torch.float32)\n",
    "\n",
    "    df_dict = dict.fromkeys(X.columns, '')\n",
    "    X.rename(columns = df_dict)\n",
    "    s1=X.iloc[:,0:4].values\n",
    "    x_tensor = torch.tensor(s1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    XScalada = scaler.fit_transform(x_tensor).type(torch.float32)\n",
    "    \n",
    "    self.data = torch.cat((XScalada,y_tensor),1)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "\n",
    "    preds = self.data[idx, 0:4]\n",
    "    spcs = self.data[idx, 4:]\n",
    "    sample = (preds, spcs)\n",
    "    \n",
    "    if self.transform:\n",
    "      sample = self.transform(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.8977,  1.0286, -1.3368, -1.3086]), tensor([1., 0., 0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = IrisDataset(\"iris.data\",\".\")\n",
    "display(dataset[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# División en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tam dataset: 150 train: 120 tamVal: 30\n"
     ]
    }
   ],
   "source": [
    "lonxitudeDataset = len(dataset)\n",
    "\n",
    "tamTrain =int(lonxitudeDataset*0.8)\n",
    "tamVal = int(lonxitudeDataset*0.2)\n",
    "\n",
    "print(f\"Tam dataset: {lonxitudeDataset} train: {tamTrain} tamVal: {tamVal}\")\n",
    "\n",
    "train_set, val_set = random_split(dataset,[tamTrain,tamVal])\n",
    "\n",
    "train_ldr = torch.utils.data.DataLoader(train_set, batch_size=2,\n",
    "    shuffle=True, drop_last=False)\n",
    "\n",
    "validation_loader =torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(in_features=50, out_features=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciación del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=4, out_features=50, bias=True)\n",
       "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (layer3): Linear(in_features=50, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model     = Model(4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2600,  0.7980, -1.2234, -1.3086],\n",
       "        [-0.0523,  2.1818, -1.4501, -1.3086]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desexada:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3903, 0.2545, 0.3552],\n",
       "        [0.4083, 0.2767, 0.3151]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0343, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradaProba,dest = next(iter(train_ldr))\n",
    "\n",
    "print(\"Entrada:\")\n",
    "display(entradaProba)\n",
    "\n",
    "print(\"Desexada:\")\n",
    "display(dest)\n",
    "\n",
    "saida = model(entradaProba) # esta é a proba de verdade\n",
    "\n",
    "print(\"Saída:\")\n",
    "display(saida)\n",
    "\n",
    "loss_fn(saida, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    # usamos enumerate para saber en que batch imos\n",
    "    for i, data in enumerate(train_ldr):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.059256410598755\n",
      "  batch 20 loss: 1.0138394832611084\n",
      "  batch 30 loss: 0.9566249966621398\n",
      "  batch 40 loss: 0.9900769889354706\n",
      "  batch 50 loss: 0.95292147397995\n",
      "  batch 60 loss: 0.9077862739562989\n",
      "LOSS train 0.9077862739562989 valid 0.9264757037162781 27.0/30\n",
      "\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.8426261305809021\n",
      "  batch 20 loss: 0.9216407835483551\n",
      "  batch 30 loss: 0.8267547905445098\n",
      "  batch 40 loss: 0.7912863552570343\n",
      "  batch 50 loss: 0.73655064702034\n",
      "  batch 60 loss: 0.7452371716499329\n",
      "LOSS train 0.7452371716499329 valid 0.8365085124969482 29.0/30\n",
      "\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.7483673453330993\n",
      "  batch 20 loss: 0.7532182693481445\n",
      "  batch 30 loss: 0.7983941018581391\n",
      "  batch 40 loss: 0.7671001315116882\n",
      "  batch 50 loss: 0.7079979538917541\n",
      "  batch 60 loss: 0.693513149023056\n",
      "LOSS train 0.693513149023056 valid 0.7696833610534668 23.0/30\n",
      "\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.6920497179031372\n",
      "  batch 20 loss: 0.784494012594223\n",
      "  batch 30 loss: 0.7066557586193085\n",
      "  batch 40 loss: 0.7042142927646637\n",
      "  batch 50 loss: 0.6409000515937805\n",
      "  batch 60 loss: 0.6681714594364166\n",
      "LOSS train 0.6681714594364166 valid 0.7329205274581909 25.0/30\n",
      "\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.6087968170642852\n",
      "  batch 20 loss: 0.643455046415329\n",
      "  batch 30 loss: 0.7214048147201538\n",
      "  batch 40 loss: 0.6939021587371826\n",
      "  batch 50 loss: 0.7326264321804047\n",
      "  batch 60 loss: 0.6037199378013611\n",
      "LOSS train 0.6037199378013611 valid 0.6875872611999512 23.0/30\n",
      "\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.7050339937210083\n",
      "  batch 20 loss: 0.6340124249458313\n",
      "  batch 30 loss: 0.6219151139259338\n",
      "  batch 40 loss: 0.654232507944107\n",
      "  batch 50 loss: 0.6146468579769134\n",
      "  batch 60 loss: 0.607133972644806\n",
      "LOSS train 0.607133972644806 valid 0.6665597558021545 21.0/30\n",
      "\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.6434540450572968\n",
      "  batch 20 loss: 0.6252562820911407\n",
      "  batch 30 loss: 0.5904392600059509\n",
      "  batch 40 loss: 0.6054213225841523\n",
      "  batch 50 loss: 0.6449582040309906\n",
      "  batch 60 loss: 0.609756463766098\n",
      "LOSS train 0.609756463766098 valid 0.6465495824813843 19.0/30\n",
      "\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.5946497082710266\n",
      "  batch 20 loss: 0.5977087378501892\n",
      "  batch 30 loss: 0.6283483386039734\n",
      "  batch 40 loss: 0.588235741853714\n",
      "  batch 50 loss: 0.6539271593093872\n",
      "  batch 60 loss: 0.5709080100059509\n",
      "LOSS train 0.5709080100059509 valid 0.6440494656562805 21.0/30\n",
      "\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.5707649469375611\n",
      "  batch 20 loss: 0.5879252016544342\n",
      "  batch 30 loss: 0.6028974235057831\n",
      "  batch 40 loss: 0.6098699450492859\n",
      "  batch 50 loss: 0.5708994567394257\n",
      "  batch 60 loss: 0.6386122345924378\n",
      "LOSS train 0.6386122345924378 valid 0.6386919617652893 25.0/30\n",
      "\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.6158330261707305\n",
      "  batch 20 loss: 0.6013321459293366\n",
      "  batch 30 loss: 0.593748676776886\n",
      "  batch 40 loss: 0.5942384362220764\n",
      "  batch 50 loss: 0.5930576503276825\n",
      "  batch 60 loss: 0.5732705056667328\n",
      "LOSS train 0.5732705056667328 valid 0.6376128196716309 21.0/30\n",
      "\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.6135085165500641\n",
      "  batch 20 loss: 0.5897424876689911\n",
      "  batch 30 loss: 0.5627447187900543\n",
      "  batch 40 loss: 0.5862414479255676\n",
      "  batch 50 loss: 0.5787625968456268\n",
      "  batch 60 loss: 0.5987631380558014\n",
      "LOSS train 0.5987631380558014 valid 0.6320865154266357 21.0/30\n",
      "\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.5790921688079834\n",
      "  batch 20 loss: 0.627637404203415\n",
      "  batch 30 loss: 0.5624310612678528\n",
      "  batch 40 loss: 0.5905517995357513\n",
      "  batch 50 loss: 0.6189551889896393\n",
      "  batch 60 loss: 0.5550144791603089\n",
      "LOSS train 0.5550144791603089 valid 0.6328315734863281 21.0/30\n",
      "\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.5645699441432953\n",
      "  batch 20 loss: 0.6024722635746003\n",
      "  batch 30 loss: 0.6047541618347168\n",
      "  batch 40 loss: 0.5700303435325622\n",
      "  batch 50 loss: 0.5686562418937683\n",
      "  batch 60 loss: 0.5826507329940795\n",
      "LOSS train 0.5826507329940795 valid 0.6293087601661682 21.0/30\n",
      "\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.5649345338344574\n",
      "  batch 20 loss: 0.5619704246520996\n",
      "  batch 30 loss: 0.556605875492096\n",
      "  batch 40 loss: 0.5553726375102996\n",
      "  batch 50 loss: 0.6413619697093964\n",
      "  batch 60 loss: 0.593111228942871\n",
      "LOSS train 0.593111228942871 valid 0.6270979642868042 25.0/30\n",
      "\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.567621910572052\n",
      "  batch 20 loss: 0.560254693031311\n",
      "  batch 30 loss: 0.5622063517570496\n",
      "  batch 40 loss: 0.6174658536911011\n",
      "  batch 50 loss: 0.6053172707557678\n",
      "  batch 60 loss: 0.5579692661762238\n",
      "LOSS train 0.5579692661762238 valid 0.6282225847244263 25.0/30\n",
      "\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.5758144736289978\n",
      "  batch 20 loss: 0.5930355310440063\n",
      "  batch 30 loss: 0.5534175157546997\n",
      "  batch 40 loss: 0.5565193355083465\n",
      "  batch 50 loss: 0.5950785160064698\n",
      "  batch 60 loss: 0.5741372287273407\n",
      "LOSS train 0.5741372287273407 valid 0.6275503635406494 25.0/30\n",
      "\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.5802615642547607\n",
      "  batch 20 loss: 0.5546492397785187\n",
      "  batch 30 loss: 0.5719942927360535\n",
      "  batch 40 loss: 0.5572564542293549\n",
      "  batch 50 loss: 0.5639177143573761\n",
      "  batch 60 loss: 0.6106071531772613\n",
      "LOSS train 0.6106071531772613 valid 0.6255834102630615 21.0/30\n",
      "\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.5541772246360779\n",
      "  batch 20 loss: 0.574018371105194\n",
      "  batch 30 loss: 0.5603970646858215\n",
      "  batch 40 loss: 0.554853492975235\n",
      "  batch 50 loss: 0.58979452252388\n",
      "  batch 60 loss: 0.5916034817695618\n",
      "LOSS train 0.5916034817695618 valid 0.6237737536430359 25.0/30\n",
      "\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.5713366150856019\n",
      "  batch 20 loss: 0.5582882225513458\n",
      "  batch 30 loss: 0.5916507840156555\n",
      "  batch 40 loss: 0.5671062052249909\n",
      "  batch 50 loss: 0.5530473589897156\n",
      "  batch 60 loss: 0.5912251889705658\n",
      "LOSS train 0.5912251889705658 valid 0.6248661279678345 25.0/30\n",
      "\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.5569750666618347\n",
      "  batch 20 loss: 0.5858497440814971\n",
      "  batch 30 loss: 0.5738529682159423\n",
      "  batch 40 loss: 0.5715617597103119\n",
      "  batch 50 loss: 0.5581009566783905\n",
      "  batch 60 loss: 0.5687076866626739\n",
      "LOSS train 0.5687076866626739 valid 0.6251468062400818 21.0/30\n",
      "\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.5667971312999726\n",
      "  batch 20 loss: 0.5546374619007111\n",
      "  batch 30 loss: 0.5625632345676422\n",
      "  batch 40 loss: 0.5538687884807587\n",
      "  batch 50 loss: 0.5993470788002014\n",
      "  batch 60 loss: 0.5843871355056762\n",
      "LOSS train 0.5843871355056762 valid 0.6220706105232239 25.0/30\n",
      "\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.5519925892353058\n",
      "  batch 20 loss: 0.5829193115234375\n",
      "  batch 30 loss: 0.6201662182807922\n",
      "  batch 40 loss: 0.5563805103302002\n",
      "  batch 50 loss: 0.5692439615726471\n",
      "  batch 60 loss: 0.5540564239025116\n",
      "LOSS train 0.5540564239025116 valid 0.6245861649513245 21.0/30\n",
      "\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.5897478044033051\n",
      "  batch 20 loss: 0.5755575478076935\n",
      "  batch 30 loss: 0.5731763482093811\n",
      "  batch 40 loss: 0.5654937744140625\n",
      "  batch 50 loss: 0.5646549046039582\n",
      "  batch 60 loss: 0.5572201430797576\n",
      "LOSS train 0.5572201430797576 valid 0.627889096736908 21.0/30\n",
      "\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.5533822476863861\n",
      "  batch 20 loss: 0.5589871823787689\n",
      "  batch 30 loss: 0.5843117892742157\n",
      "  batch 40 loss: 0.559562623500824\n",
      "  batch 50 loss: 0.5564224123954773\n",
      "  batch 60 loss: 0.5974658787250519\n",
      "LOSS train 0.5974658787250519 valid 0.6246261596679688 25.0/30\n",
      "\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.5541909396648407\n",
      "  batch 20 loss: 0.5662586271762848\n",
      "  batch 30 loss: 0.6025046646595001\n",
      "  batch 40 loss: 0.5546324610710144\n",
      "  batch 50 loss: 0.5765727996826172\n",
      "  batch 60 loss: 0.5628133118152618\n",
      "LOSS train 0.5628133118152618 valid 0.6245386600494385 25.0/30\n",
      "\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.5883139073848724\n",
      "  batch 20 loss: 0.5545886158943176\n",
      "  batch 30 loss: 0.5876326560974121\n",
      "  batch 40 loss: 0.5613381326198578\n",
      "  batch 50 loss: 0.5519331336021424\n",
      "  batch 60 loss: 0.5578909277915954\n",
      "LOSS train 0.5578909277915954 valid 0.6276063919067383 21.0/30\n",
      "\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.5805548369884491\n",
      "  batch 20 loss: 0.5525563299655915\n",
      "  batch 30 loss: 0.5689269185066224\n",
      "  batch 40 loss: 0.558634489774704\n",
      "  batch 50 loss: 0.5525512397289276\n",
      "  batch 60 loss: 0.6137001156806946\n",
      "LOSS train 0.6137001156806946 valid 0.6274747848510742 21.0/30\n",
      "\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.554806137084961\n",
      "  batch 20 loss: 0.5558587551116944\n",
      "  batch 30 loss: 0.570816445350647\n",
      "  batch 40 loss: 0.5771374583244324\n",
      "  batch 50 loss: 0.5719761848449707\n",
      "  batch 60 loss: 0.5817368566989899\n",
      "LOSS train 0.5817368566989899 valid 0.6207928657531738 25.0/30\n",
      "\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.5849718987941742\n",
      "  batch 20 loss: 0.5558131992816925\n",
      "  batch 30 loss: 0.5534501791000366\n",
      "  batch 40 loss: 0.553686898946762\n",
      "  batch 50 loss: 0.5542753458023071\n",
      "  batch 60 loss: 0.5971641659736633\n",
      "LOSS train 0.5971641659736633 valid 0.6220489144325256 21.0/30\n",
      "\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.5523476898670197\n",
      "  batch 20 loss: 0.5521505653858185\n",
      "  batch 30 loss: 0.5582325160503387\n",
      "  batch 40 loss: 0.5897386610507965\n",
      "  batch 50 loss: 0.5568166136741638\n",
      "  batch 60 loss: 0.5784240782260894\n",
      "LOSS train 0.5784240782260894 valid 0.621826171875 25.0/30\n",
      "\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.5570863127708435\n",
      "  batch 20 loss: 0.5951232075691223\n",
      "  batch 30 loss: 0.5559009909629822\n",
      "  batch 40 loss: 0.5523611724376678\n",
      "  batch 50 loss: 0.5529575228691102\n",
      "  batch 60 loss: 0.5895732998847961\n",
      "LOSS train 0.5895732998847961 valid 0.6228930354118347 21.0/30\n",
      "\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.554857611656189\n",
      "  batch 20 loss: 0.5576758205890655\n",
      "  batch 30 loss: 0.5561976194381714\n",
      "  batch 40 loss: 0.5802399933338165\n",
      "  batch 50 loss: 0.57335165143013\n",
      "  batch 60 loss: 0.5656152665615082\n",
      "LOSS train 0.5656152665615082 valid 0.622517466545105 25.0/30\n",
      "\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.5553133189678192\n",
      "  batch 20 loss: 0.5527330875396729\n",
      "  batch 30 loss: 0.5620827853679657\n",
      "  batch 40 loss: 0.5543489813804626\n",
      "  batch 50 loss: 0.553044980764389\n",
      "  batch 60 loss: 0.6154448926448822\n",
      "LOSS train 0.6154448926448822 valid 0.6226111650466919 25.0/30\n",
      "\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.5692721784114838\n",
      "  batch 20 loss: 0.5545610725879669\n",
      "  batch 30 loss: 0.5516693651676178\n",
      "  batch 40 loss: 0.5842804729938507\n",
      "  batch 50 loss: 0.5668475031852722\n",
      "  batch 60 loss: 0.5700487554073334\n",
      "LOSS train 0.5700487554073334 valid 0.6264476776123047 25.0/30\n",
      "\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.5562654435634613\n",
      "  batch 20 loss: 0.5571410179138183\n",
      "  batch 30 loss: 0.5527195513248444\n",
      "  batch 40 loss: 0.5881086349487304\n",
      "  batch 50 loss: 0.5527783453464508\n",
      "  batch 60 loss: 0.5839777588844299\n",
      "LOSS train 0.5839777588844299 valid 0.6219685673713684 25.0/30\n",
      "\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.5641323089599609\n",
      "  batch 20 loss: 0.5603321969509125\n",
      "  batch 30 loss: 0.5690756440162659\n",
      "  batch 40 loss: 0.5624060451984405\n",
      "  batch 50 loss: 0.5640668570995331\n",
      "  batch 60 loss: 0.5552751362323761\n",
      "LOSS train 0.5552751362323761 valid 0.6392059922218323 21.0/30\n",
      "\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.5789352059364319\n",
      "  batch 20 loss: 0.5551257729530334\n",
      "  batch 30 loss: 0.5530726552009583\n",
      "  batch 40 loss: 0.573921263217926\n",
      "  batch 50 loss: 0.5715305984020234\n",
      "  batch 60 loss: 0.5582037568092346\n",
      "LOSS train 0.5582037568092346 valid 0.6326395273208618 25.0/30\n",
      "\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.5678238570690155\n",
      "  batch 20 loss: 0.5790265560150146\n",
      "  batch 30 loss: 0.5603039681911468\n",
      "  batch 40 loss: 0.5552618324756622\n",
      "  batch 50 loss: 0.5548172950744629\n",
      "  batch 60 loss: 0.5519356191158294\n",
      "LOSS train 0.5519356191158294 valid 0.6358694434165955 25.0/30\n",
      "\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.551791387796402\n",
      "  batch 20 loss: 0.5550795435905457\n",
      "  batch 30 loss: 0.5787534594535828\n",
      "  batch 40 loss: 0.5517508625984192\n",
      "  batch 50 loss: 0.5659945487976075\n",
      "  batch 60 loss: 0.5747852444648742\n",
      "LOSS train 0.5747852444648742 valid 0.6332467794418335 25.0/30\n",
      "\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.5743414044380188\n",
      "  batch 20 loss: 0.5614638388156891\n",
      "  batch 30 loss: 0.5571244597434998\n",
      "  batch 40 loss: 0.569686371088028\n",
      "  batch 50 loss: 0.5523059606552124\n",
      "  batch 60 loss: 0.5626192808151245\n",
      "LOSS train 0.5626192808151245 valid 0.633725106716156 25.0/30\n",
      "\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.5529484212398529\n",
      "  batch 20 loss: 0.5545717060565949\n",
      "  batch 30 loss: 0.5820438444614411\n",
      "  batch 40 loss: 0.551869523525238\n",
      "  batch 50 loss: 0.5616081893444062\n",
      "  batch 60 loss: 0.5580464422702789\n",
      "LOSS train 0.5580464422702789 valid 0.6263235807418823 25.0/30\n",
      "\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.5537941098213196\n",
      "  batch 20 loss: 0.5536714375019074\n",
      "  batch 30 loss: 0.5520652949810028\n",
      "  batch 40 loss: 0.597257673740387\n",
      "  batch 50 loss: 0.5634620904922485\n",
      "  batch 60 loss: 0.557021415233612\n",
      "LOSS train 0.557021415233612 valid 0.6262040138244629 25.0/30\n",
      "\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.5517439723014832\n",
      "  batch 20 loss: 0.5526393294334412\n",
      "  batch 30 loss: 0.584350049495697\n",
      "  batch 40 loss: 0.5854411005973816\n",
      "  batch 50 loss: 0.5540967226028443\n",
      "  batch 60 loss: 0.5575042963027954\n",
      "LOSS train 0.5575042963027954 valid 0.6461278200149536 25.0/30\n",
      "\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.5536434054374695\n",
      "  batch 20 loss: 0.5575099527835846\n",
      "  batch 30 loss: 0.5518311619758606\n",
      "  batch 40 loss: 0.5868416786193847\n",
      "  batch 50 loss: 0.561862176656723\n",
      "  batch 60 loss: 0.5544054806232452\n",
      "LOSS train 0.5544054806232452 valid 0.6278997659683228 25.0/30\n",
      "\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.5642204999923706\n",
      "  batch 20 loss: 0.5524631679058075\n",
      "  batch 30 loss: 0.5658767104148865\n",
      "  batch 40 loss: 0.5517284452915192\n",
      "  batch 50 loss: 0.5520434319972992\n",
      "  batch 60 loss: 0.5710581362247467\n",
      "LOSS train 0.5710581362247467 valid 0.6269737482070923 25.0/30\n",
      "\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.5569112420082092\n",
      "  batch 20 loss: 0.5652018010616302\n",
      "  batch 30 loss: 0.567406177520752\n",
      "  batch 40 loss: 0.5572925746440888\n",
      "  batch 50 loss: 0.5518875181674957\n",
      "  batch 60 loss: 0.5516580879688263\n",
      "LOSS train 0.5516580879688263 valid 0.6337617635726929 25.0/30\n",
      "\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.5524516940116883\n",
      "  batch 20 loss: 0.5518155336380005\n",
      "  batch 30 loss: 0.5529594659805298\n",
      "  batch 40 loss: 0.5520597398281097\n",
      "  batch 50 loss: 0.5750017404556275\n",
      "  batch 60 loss: 0.5665468871593475\n",
      "LOSS train 0.5665468871593475 valid 0.6219422817230225 25.0/30\n",
      "\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.5663862764835358\n",
      "  batch 20 loss: 0.567015391588211\n",
      "  batch 30 loss: 0.5517692148685456\n",
      "  batch 40 loss: 0.5535604596138001\n",
      "  batch 50 loss: 0.5517418444156647\n",
      "  batch 60 loss: 0.5569056510925293\n",
      "LOSS train 0.5569056510925293 valid 0.6326828598976135 25.0/30\n",
      "\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.5516148328781127\n",
      "  batch 20 loss: 0.5542389154434204\n",
      "  batch 30 loss: 0.5792325615882874\n",
      "  batch 40 loss: 0.5600570321083069\n",
      "  batch 50 loss: 0.5548382222652435\n",
      "  batch 60 loss: 0.5628422737121582\n",
      "LOSS train 0.5628422737121582 valid 0.626968264579773 25.0/30\n",
      "\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.5515540957450866\n",
      "  batch 20 loss: 0.5518791317939759\n",
      "  batch 30 loss: 0.5711749970912934\n",
      "  batch 40 loss: 0.5561433255672454\n",
      "  batch 50 loss: 0.5518527090549469\n",
      "  batch 60 loss: 0.5636740326881409\n",
      "LOSS train 0.5636740326881409 valid 0.6281110644340515 25.0/30\n",
      "\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.5608113825321197\n",
      "  batch 20 loss: 0.5541625499725342\n",
      "  batch 30 loss: 0.573700624704361\n",
      "  batch 40 loss: 0.5520091712474823\n",
      "  batch 50 loss: 0.5524514675140381\n",
      "  batch 60 loss: 0.5520286440849305\n",
      "LOSS train 0.5520286440849305 valid 0.6304517984390259 25.0/30\n",
      "\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.5644949078559875\n",
      "  batch 20 loss: 0.5635268986225128\n",
      "  batch 30 loss: 0.5526674568653107\n",
      "  batch 40 loss: 0.5618633806705475\n",
      "  batch 50 loss: 0.5520496785640716\n",
      "  batch 60 loss: 0.5538931608200073\n",
      "LOSS train 0.5538931608200073 valid 0.6267118453979492 25.0/30\n",
      "\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.5521060466766358\n",
      "  batch 20 loss: 0.551693606376648\n",
      "  batch 30 loss: 0.5651044547557831\n",
      "  batch 40 loss: 0.5574043393135071\n",
      "  batch 50 loss: 0.5606261670589447\n",
      "  batch 60 loss: 0.5553498864173889\n",
      "LOSS train 0.5553498864173889 valid 0.6256905198097229 25.0/30\n",
      "\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.5548409759998322\n",
      "  batch 20 loss: 0.5645800411701203\n",
      "  batch 30 loss: 0.5529756784439087\n",
      "  batch 40 loss: 0.5517487347126007\n",
      "  batch 50 loss: 0.5517856657505036\n",
      "  batch 60 loss: 0.5638829052448273\n",
      "LOSS train 0.5638829052448273 valid 0.6264387369155884 25.0/30\n",
      "\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.5518827497959137\n",
      "  batch 20 loss: 0.5573878943920135\n",
      "  batch 30 loss: 0.5560929000377655\n",
      "  batch 40 loss: 0.5627980887889862\n",
      "  batch 50 loss: 0.5515342593193054\n",
      "  batch 60 loss: 0.5548452734947205\n",
      "LOSS train 0.5548452734947205 valid 0.6314023733139038 25.0/30\n",
      "\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.5626859366893768\n",
      "  batch 20 loss: 0.5530839204788208\n",
      "  batch 30 loss: 0.5544429779052734\n",
      "  batch 40 loss: 0.553763234615326\n",
      "  batch 50 loss: 0.560951167345047\n",
      "  batch 60 loss: 0.5516642034053802\n",
      "LOSS train 0.5516642034053802 valid 0.6255490779876709 25.0/30\n",
      "\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.5573241949081421\n",
      "  batch 20 loss: 0.560680490732193\n",
      "  batch 30 loss: 0.5514974653720855\n",
      "  batch 40 loss: 0.5522283732891082\n",
      "  batch 50 loss: 0.551816338300705\n",
      "  batch 60 loss: 0.5567816317081451\n",
      "LOSS train 0.5567816317081451 valid 0.633297860622406 25.0/30\n",
      "\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.5514985024929047\n",
      "  batch 20 loss: 0.5672802567481995\n",
      "  batch 30 loss: 0.5532280504703522\n",
      "  batch 40 loss: 0.5583664536476135\n",
      "  batch 50 loss: 0.5587298929691314\n",
      "  batch 60 loss: 0.5518032014369965\n",
      "LOSS train 0.5518032014369965 valid 0.6365852355957031 25.0/30\n",
      "\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.5514744818210602\n",
      "  batch 20 loss: 0.552489572763443\n",
      "  batch 30 loss: 0.55181125998497\n",
      "  batch 40 loss: 0.5637958467006683\n",
      "  batch 50 loss: 0.5609736680984497\n",
      "  batch 60 loss: 0.5545847713947296\n",
      "LOSS train 0.5545847713947296 valid 0.6228500604629517 25.0/30\n",
      "\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.5576773524284363\n",
      "  batch 20 loss: 0.5578641831874848\n",
      "  batch 30 loss: 0.5529228627681733\n",
      "  batch 40 loss: 0.5601272404193878\n",
      "  batch 50 loss: 0.5514916181564331\n",
      "  batch 60 loss: 0.5519714891910553\n",
      "LOSS train 0.5519714891910553 valid 0.6257892847061157 25.0/30\n",
      "\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.5594455599784851\n",
      "  batch 20 loss: 0.5517945230007172\n",
      "  batch 30 loss: 0.5525706589221955\n",
      "  batch 40 loss: 0.5542481482028961\n",
      "  batch 50 loss: 0.5569602727890015\n",
      "  batch 60 loss: 0.5549494802951813\n",
      "LOSS train 0.5549494802951813 valid 0.6320134401321411 25.0/30\n",
      "\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.5546039998531341\n",
      "  batch 20 loss: 0.5612277030944824\n",
      "  batch 30 loss: 0.5563155055046082\n",
      "  batch 40 loss: 0.552601820230484\n",
      "  batch 50 loss: 0.5532196164131165\n",
      "  batch 60 loss: 0.5528908491134643\n",
      "LOSS train 0.5528908491134643 valid 0.6327863931655884 25.0/30\n",
      "\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.5529920935630799\n",
      "  batch 20 loss: 0.5524405121803284\n",
      "  batch 30 loss: 0.5583995044231415\n",
      "  batch 40 loss: 0.5518765151500702\n",
      "  batch 50 loss: 0.5590362250804901\n",
      "  batch 60 loss: 0.5553290188312531\n",
      "LOSS train 0.5553290188312531 valid 0.6257269382476807 25.0/30\n",
      "\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.5554571688175202\n",
      "  batch 20 loss: 0.5522545039653778\n",
      "  batch 30 loss: 0.552427452802658\n",
      "  batch 40 loss: 0.5640441358089447\n",
      "  batch 50 loss: 0.5554821968078614\n",
      "  batch 60 loss: 0.5514926493167878\n",
      "LOSS train 0.5514926493167878 valid 0.6242547035217285 25.0/30\n",
      "\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.5532327592372894\n",
      "  batch 20 loss: 0.5517967939376831\n",
      "  batch 30 loss: 0.5566097259521484\n",
      "  batch 40 loss: 0.5572918593883515\n",
      "  batch 50 loss: 0.5532095968723297\n",
      "  batch 60 loss: 0.5526305437088013\n",
      "LOSS train 0.5526305437088013 valid 0.6419293880462646 25.0/30\n",
      "\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.5516802906990051\n",
      "  batch 20 loss: 0.5522135853767395\n",
      "  batch 30 loss: 0.5615739405155182\n",
      "  batch 40 loss: 0.5577368974685669\n",
      "  batch 50 loss: 0.5526482403278351\n",
      "  batch 60 loss: 0.5649090766906738\n",
      "LOSS train 0.5649090766906738 valid 0.6397823095321655 25.0/30\n",
      "\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.5529580056667328\n",
      "  batch 20 loss: 0.5768436670303345\n",
      "  batch 30 loss: 0.5514866828918457\n",
      "  batch 40 loss: 0.5515658915042877\n",
      "  batch 50 loss: 0.557696932554245\n",
      "  batch 60 loss: 0.5573486626148224\n",
      "LOSS train 0.5573486626148224 valid 0.6360965967178345 25.0/30\n",
      "\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.5523693919181824\n",
      "  batch 20 loss: 0.5547175884246827\n",
      "  batch 30 loss: 0.5519156396389008\n",
      "  batch 40 loss: 0.5578627169132233\n",
      "  batch 50 loss: 0.5530952870845794\n",
      "  batch 60 loss: 0.5517993450164795\n",
      "LOSS train 0.5517993450164795 valid 0.6357516646385193 25.0/30\n",
      "\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.5564381301403045\n",
      "  batch 20 loss: 0.5537093818187714\n",
      "  batch 30 loss: 0.5534642279148102\n",
      "  batch 40 loss: 0.5524416744709015\n",
      "  batch 50 loss: 0.5546020805835724\n",
      "  batch 60 loss: 0.5520321011543274\n",
      "LOSS train 0.5520321011543274 valid 0.6391022801399231 25.0/30\n",
      "\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.553591275215149\n",
      "  batch 20 loss: 0.5514732658863067\n",
      "  batch 30 loss: 0.5516189336776733\n",
      "  batch 40 loss: 0.5609202265739441\n",
      "  batch 50 loss: 0.5551219701766967\n",
      "  batch 60 loss: 0.5514570474624634\n",
      "LOSS train 0.5514570474624634 valid 0.6231337189674377 25.0/30\n",
      "\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.5548981904983521\n",
      "  batch 20 loss: 0.5517812252044678\n",
      "  batch 30 loss: 0.5550661385059357\n",
      "  batch 40 loss: 0.5531375765800476\n",
      "  batch 50 loss: 0.5515350997447968\n",
      "  batch 60 loss: 0.5544908463954925\n",
      "LOSS train 0.5544908463954925 valid 0.6411885023117065 25.0/30\n",
      "\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.55227010846138\n",
      "  batch 20 loss: 0.5515032947063446\n",
      "  batch 30 loss: 0.551902437210083\n",
      "  batch 40 loss: 0.5643592536449432\n",
      "  batch 50 loss: 0.5583689272403717\n",
      "  batch 60 loss: 0.5527889788150787\n",
      "LOSS train 0.5527889788150787 valid 0.622959554195404 25.0/30\n",
      "\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.5517992973327637\n",
      "  batch 20 loss: 0.5530717551708222\n",
      "  batch 30 loss: 0.5535780727863312\n",
      "  batch 40 loss: 0.5514512538909913\n",
      "  batch 50 loss: 0.5556040227413177\n",
      "  batch 60 loss: 0.5575330317020416\n",
      "LOSS train 0.5575330317020416 valid 0.6265182495117188 25.0/30\n",
      "\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.5549623191356658\n",
      "  batch 20 loss: 0.5546272456645965\n",
      "  batch 30 loss: 0.5541973888874054\n",
      "  batch 40 loss: 0.5528590559959412\n",
      "  batch 50 loss: 0.5516754448413849\n",
      "  batch 60 loss: 0.5514576971530915\n",
      "LOSS train 0.5514576971530915 valid 0.6301681399345398 25.0/30\n",
      "\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.5516430079936981\n",
      "  batch 20 loss: 0.5526795148849487\n",
      "  batch 30 loss: 0.551453822851181\n",
      "  batch 40 loss: 0.554692167043686\n",
      "  batch 50 loss: 0.5515233635902405\n",
      "  batch 60 loss: 0.5558756649494171\n",
      "LOSS train 0.5558756649494171 valid 0.6322634816169739 25.0/30\n",
      "\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.5517597496509552\n",
      "  batch 20 loss: 0.5528625428676606\n",
      "  batch 30 loss: 0.5558857202529908\n",
      "  batch 40 loss: 0.553617137670517\n",
      "  batch 50 loss: 0.5523444533348083\n",
      "  batch 60 loss: 0.5515567839145661\n",
      "LOSS train 0.5515567839145661 valid 0.6319207549095154 25.0/30\n",
      "\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.5537781953811646\n",
      "  batch 20 loss: 0.5546600878238678\n",
      "  batch 30 loss: 0.5522799551486969\n",
      "  batch 40 loss: 0.5514527499675751\n",
      "  batch 50 loss: 0.553354161977768\n",
      "  batch 60 loss: 0.5515198230743408\n",
      "LOSS train 0.5515198230743408 valid 0.637643575668335 25.0/30\n",
      "\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.5518563508987426\n",
      "  batch 20 loss: 0.5539074897766113\n",
      "  batch 30 loss: 0.5516290724277496\n",
      "  batch 40 loss: 0.5524258553981781\n",
      "  batch 50 loss: 0.5526218771934509\n",
      "  batch 60 loss: 0.5553972959518433\n",
      "LOSS train 0.5553972959518433 valid 0.6370465755462646 25.0/30\n",
      "\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.5518589377403259\n",
      "  batch 20 loss: 0.5531939029693603\n",
      "  batch 30 loss: 0.5514570653438569\n",
      "  batch 40 loss: 0.552512812614441\n",
      "  batch 50 loss: 0.5555005252361298\n",
      "  batch 60 loss: 0.552207636833191\n",
      "LOSS train 0.552207636833191 valid 0.6350680589675903 25.0/30\n",
      "\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.5524282276630401\n",
      "  batch 20 loss: 0.551952701807022\n",
      "  batch 30 loss: 0.5514618396759033\n",
      "  batch 40 loss: 0.5523412108421326\n",
      "  batch 50 loss: 0.5544454872608184\n",
      "  batch 60 loss: 0.5542495846748352\n",
      "LOSS train 0.5542495846748352 valid 0.6291773319244385 25.0/30\n",
      "\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.5527393162250519\n",
      "  batch 20 loss: 0.5519133031368255\n",
      "  batch 30 loss: 0.5549569487571716\n",
      "  batch 40 loss: 0.5514530062675476\n",
      "  batch 50 loss: 0.5519206643104553\n",
      "  batch 60 loss: 0.5550821781158447\n",
      "LOSS train 0.5550821781158447 valid 0.6411147117614746 25.0/30\n",
      "\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.5522497594356537\n",
      "  batch 20 loss: 0.5528638958930969\n",
      "  batch 30 loss: 0.5520978629589081\n",
      "  batch 40 loss: 0.5535332977771759\n",
      "  batch 50 loss: 0.5531615376472473\n",
      "  batch 60 loss: 0.551459264755249\n",
      "LOSS train 0.551459264755249 valid 0.6345494985580444 25.0/30\n",
      "\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 0.5520765602588653\n",
      "  batch 20 loss: 0.554052722454071\n",
      "  batch 30 loss: 0.5528809905052186\n",
      "  batch 40 loss: 0.5520522475242615\n",
      "  batch 50 loss: 0.5514568388462067\n",
      "  batch 60 loss: 0.5524695873260498\n",
      "LOSS train 0.5524695873260498 valid 0.6363946795463562 25.0/30\n",
      "\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.5526237785816193\n",
      "  batch 20 loss: 0.5517224848270417\n",
      "  batch 30 loss: 0.5522292733192444\n",
      "  batch 40 loss: 0.5539794027805328\n",
      "  batch 50 loss: 0.552367913722992\n",
      "  batch 60 loss: 0.5517879486083984\n",
      "LOSS train 0.5517879486083984 valid 0.6385924220085144 25.0/30\n",
      "\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.5514676868915558\n",
      "  batch 20 loss: 0.5516499280929565\n",
      "  batch 30 loss: 0.5550080597400665\n",
      "  batch 40 loss: 0.5530965745449066\n",
      "  batch 50 loss: 0.5528414070606231\n",
      "  batch 60 loss: 0.5526398181915283\n",
      "LOSS train 0.5526398181915283 valid 0.6363335847854614 25.0/30\n",
      "\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.5548171877861023\n",
      "  batch 20 loss: 0.5515002250671387\n",
      "  batch 30 loss: 0.5515452861785889\n",
      "  batch 40 loss: 0.5520787358283996\n",
      "  batch 50 loss: 0.5519124805927277\n",
      "  batch 60 loss: 0.5520987570285797\n",
      "LOSS train 0.5520987570285797 valid 0.6404612064361572 25.0/30\n",
      "\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.5534082531929017\n",
      "  batch 20 loss: 0.5514685809612274\n",
      "  batch 30 loss: 0.5519054174423218\n",
      "  batch 40 loss: 0.5523987591266633\n",
      "  batch 50 loss: 0.55237677693367\n",
      "  batch 60 loss: 0.5540436804294586\n",
      "LOSS train 0.5540436804294586 valid 0.6381328105926514 25.0/30\n",
      "\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.5514547526836395\n",
      "  batch 20 loss: 0.5531619906425476\n",
      "  batch 30 loss: 0.5514732003211975\n",
      "  batch 40 loss: 0.5524805188179016\n",
      "  batch 50 loss: 0.5529762268066406\n",
      "  batch 60 loss: 0.553941398859024\n",
      "LOSS train 0.553941398859024 valid 0.6366918683052063 25.0/30\n",
      "\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.5551055371761322\n",
      "  batch 20 loss: 0.5517401278018952\n",
      "  batch 30 loss: 0.5518764436244965\n",
      "  batch 40 loss: 0.5517053544521332\n",
      "  batch 50 loss: 0.5515874266624451\n",
      "  batch 60 loss: 0.5514654874801636\n",
      "LOSS train 0.5514654874801636 valid 0.6403278112411499 25.0/30\n",
      "\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.551726496219635\n",
      "  batch 20 loss: 0.5514542996883393\n",
      "  batch 30 loss: 0.5516892492771148\n",
      "  batch 40 loss: 0.5540547311306\n",
      "  batch 50 loss: 0.5528913557529449\n",
      "  batch 60 loss: 0.5522850155830383\n",
      "LOSS train 0.5522850155830383 valid 0.6357327103614807 25.0/30\n",
      "\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.5520926773548126\n",
      "  batch 20 loss: 0.5520459830760955\n",
      "  batch 30 loss: 0.5527890205383301\n",
      "  batch 40 loss: 0.5527988731861114\n",
      "  batch 50 loss: 0.5516309559345245\n",
      "  batch 60 loss: 0.5522925555706024\n",
      "LOSS train 0.5522925555706024 valid 0.6390287280082703 25.0/30\n",
      "\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.5514533340930938\n",
      "  batch 20 loss: 0.5523789048194885\n",
      "  batch 30 loss: 0.5533916711807251\n",
      "  batch 40 loss: 0.5514653027057648\n",
      "  batch 50 loss: 0.5527595400810241\n",
      "  batch 60 loss: 0.5515022337436676\n",
      "LOSS train 0.5515022337436676 valid 0.6358585357666016 25.0/30\n",
      "\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.551451462507248\n",
      "  batch 20 loss: 0.5524732589721679\n",
      "  batch 30 loss: 0.5522910177707672\n",
      "  batch 40 loss: 0.5520160019397735\n",
      "  batch 50 loss: 0.5527179062366485\n",
      "  batch 60 loss: 0.5521311640739441\n",
      "LOSS train 0.5521311640739441 valid 0.6385131478309631 25.0/30\n",
      "\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.5526017785072327\n",
      "  batch 20 loss: 0.5520474195480347\n",
      "  batch 30 loss: 0.5519234299659729\n",
      "  batch 40 loss: 0.5531326472759247\n",
      "  batch 50 loss: 0.5514577567577362\n",
      "  batch 60 loss: 0.5514533042907714\n",
      "LOSS train 0.5514533042907714 valid 0.6396344900131226 25.0/30\n",
      "\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.5524529218673706\n",
      "  batch 20 loss: 0.5525029718875885\n",
      "  batch 30 loss: 0.5514582991600037\n",
      "  batch 40 loss: 0.5528047621250153\n",
      "  batch 50 loss: 0.5518633604049683\n",
      "  batch 60 loss: 0.5514623165130615\n",
      "LOSS train 0.5514623165130615 valid 0.6380190253257751 25.0/30\n",
      "\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.5528373718261719\n",
      "  batch 20 loss: 0.5522148549556732\n",
      "  batch 30 loss: 0.5523087322711945\n",
      "  batch 40 loss: 0.5515158116817475\n",
      "  batch 50 loss: 0.5515698194503784\n",
      "  batch 60 loss: 0.5516674518585205\n",
      "LOSS train 0.5516674518585205 valid 0.6403111219406128 25.0/30\n",
      "\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.5518680930137634\n",
      "  batch 20 loss: 0.5531922161579133\n",
      "  batch 30 loss: 0.5524091303348542\n",
      "  batch 40 loss: 0.5514485895633697\n",
      "  batch 50 loss: 0.5518099725246429\n",
      "  batch 60 loss: 0.5517015099525452\n",
      "LOSS train 0.5517015099525452 valid 0.6392825245857239 25.0/30\n",
      "\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.5516091585159302\n",
      "  batch 20 loss: 0.5518494367599487\n",
      "  batch 30 loss: 0.5514529168605804\n",
      "  batch 40 loss: 0.5516809523105621\n",
      "  batch 50 loss: 0.5524393558502197\n",
      "  batch 60 loss: 0.553055864572525\n",
      "LOSS train 0.553055864572525 valid 0.6419451236724854 25.0/30\n",
      "\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.551464831829071\n",
      "  batch 20 loss: 0.5515878677368165\n",
      "  batch 30 loss: 0.5528697609901428\n",
      "  batch 40 loss: 0.5518731594085693\n",
      "  batch 50 loss: 0.5514673173427582\n",
      "  batch 60 loss: 0.5525938451290131\n",
      "LOSS train 0.5525938451290131 valid 0.6403928399085999 25.0/30\n",
      "\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.551449429988861\n",
      "  batch 20 loss: 0.5519281566143036\n",
      "  batch 30 loss: 0.5516894459724426\n",
      "  batch 40 loss: 0.5522588789463043\n",
      "  batch 50 loss: 0.5529826879501343\n",
      "  batch 60 loss: 0.5514538943767547\n",
      "LOSS train 0.5514538943767547 valid 0.6393819451332092 25.0/30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "loss_list     = torch.zeros((EPOCHS,))\n",
    "accuracy_list = torch.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Poñemos o modelo en modo entrenamento\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, None)\n",
    "    loss_list[epoch] = avg_loss\n",
    "    \n",
    "    # Non se precisan os gradientes para o test\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "\n",
    "        correct = (torch.argmax(voutputs, dim=0) == vlabels).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] += correct.sum()\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {} {}/{}\\n'.format(avg_loss, avg_vloss,accuracy_list[epoch],int(lonxitudeDataset*0.2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"pesos.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
